{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from deep_translator import GoogleTranslator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\College\\Projects\\Translator\\capstone\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "train_df = pd.read_parquet(\"hf://datasets/cfilt/iitb-english-hindi/\" + splits[\"train\"])\n",
    "test_df = pd.read_parquet(\"hf://datasets/cfilt/iitb-english-hindi/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", '', text)  # Remove unwanted characters for English\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "def clean_hindi_text(text):\n",
    "    text = re.sub(r\"[^\\u0900-\\u097F\\s]\", '', text)  # Keep Hindi characters and space\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()  # Remove extra whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self):\n",
    "        self.train_df = train_df.sample(frac=0.05, random_state=1)\n",
    "        self.test_df = test_df\n",
    "        self.frames = [self.train_df, self.test_df]\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=60)\n",
    "\n",
    "\n",
    "\n",
    "    def create_df(self):\n",
    "        for i in self.frames:\n",
    "            i['data'] = i['translation'].apply(lambda x: x['en'])\n",
    "            i['target'] = i['translation'].apply(lambda x: x['hi'])\n",
    "\n",
    "    def drop_cols(self):\n",
    "        self.train_df = self.train_df.drop(columns=['translation'], axis=1)\n",
    "        self.test_df = self.test_df.drop(columns=['translation'], axis=1)\n",
    "\n",
    "    def clean_text(self):\n",
    "        for frame in self.frames:\n",
    "            frame['data'] = frame['data'].apply(clean_text)\n",
    "            frame['target'] = frame['target'].apply(clean_hindi_text)\n",
    "\n",
    "    def eng_tokenization(self):\n",
    "        self.eng_tokenizer = Tokenizer()\n",
    "        self.eng_tokenizer.fit_on_texts(self.train_df['data'])\n",
    "        self.eng_sequences = self.eng_tokenizer.texts_to_sequences(self.train_df['data'])\n",
    "    def hin_tokenization(self):\n",
    "        self.hin_tokenizer = Tokenizer()\n",
    "        self.hin_tokenizer.fit_on_texts(self.train_df['target'])\n",
    "        self.hin_sequences = self.hin_tokenizer.texts_to_sequences(self.train_df['target'])\n",
    "\n",
    "    def vocabulary_creation(self):\n",
    "        self.eng_vocab_size = len(self.eng_tokenizer.word_index) + 1\n",
    "        self.hin_vocab_size = len(self.hin_tokenizer.word_index) + 1\n",
    "\n",
    "    def seq_padding(self):\n",
    "        self.max_eng_length = max([len(seq) for seq in self.eng_sequences])\n",
    "        self.max_hin_length = max([len(seq) for seq in self.hin_sequences])\n",
    "        self.eng_sequences = pad_sequences(self.eng_sequences, maxlen=self.max_eng_length, padding='post')\n",
    "        self.hin_sequences = pad_sequences(self.hin_sequences, maxlen=self.max_hin_length, padding='post')\n",
    "\n",
    "    def split_data(self):\n",
    "        self.eng_train, self.eng_val, self.hin_train, self.hin_val = train_test_split(self.eng_sequences, self.hin_sequences, test_size=0.2)\n",
    "        # print(f\"eng_train shape: {self.eng_train.shape}\")\n",
    "        # print(f\"hin_train shape: {self.hin_train.shape}\")\n",
    "        # print(f\"eng_val shape: {self.eng_val.shape}\")\n",
    "        # print(f\"hin_val shape: {self.hin_val.shape}\")\n",
    "\n",
    "    def implement_PCA(self):\n",
    "        self.eng_train = self.scaler.fit_transform(self.eng_train)\n",
    "        self.eng_train = self.pca.fit_transform(self.eng_train)\n",
    "        \n",
    "\n",
    "    def initialize_encoder(self):\n",
    "        self.encoder_inputs = Input(shape=(self.eng_train.shape[1],))\n",
    "        encoder_embedding = Embedding(self.eng_vocab_size, 256, mask_zero=True)(self.encoder_inputs)\n",
    "        encoder_lstm = LSTM(256, return_state=True)\n",
    "        self.encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "        self.encoder_states = [state_h, state_c]\n",
    "\n",
    "    def initialize_decoder(self):\n",
    "        self.decoder_inputs = Input(shape=((self.hin_train.shape[1] -1 ),))\n",
    "        decoder_embedding = Embedding(self.hin_vocab_size, 256, mask_zero=True)(self.decoder_inputs)\n",
    "        decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=self.encoder_states)\n",
    "        decoder_dense = Dense(self.hin_vocab_size, activation='softmax')\n",
    "        self.decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    def initialize_model(self):\n",
    "        self.model = Model([self.encoder_inputs, self.decoder_inputs], self.decoder_outputs)\n",
    "        self.model.compile(optimizer=RMSprop(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(\"Model initialized and compiled.\")\n",
    "\n",
    "    def initialize_training(self):\n",
    "        try:\n",
    "            self.hin_train = np.array(self.hin_train)\n",
    "            self.hin_val = np.array(self.hin_val)\n",
    "\n",
    "            self.decoder_input_train = self.hin_train[:, :-1]\n",
    "            self.decoder_target_train = self.hin_train[:, 1:]\n",
    "            self.decoder_input_val = self.hin_val[:, :-1]\n",
    "            self.decoder_target_val = self.hin_val[:, 1:]\n",
    "\n",
    "            self.model.fit(\n",
    "                [self.eng_train, self.decoder_input_train], \n",
    "                self.decoder_target_train, \n",
    "                validation_data=([self.eng_val, self.decoder_input_val], self.decoder_target_val), \n",
    "                batch_size=64, epochs=50\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training: {e}\")\n",
    "        \n",
    "    def get_output(self, word):\n",
    "        try:\n",
    "            # Initialize the GoogleTranslator for English to Hindi\n",
    "            translator = GoogleTranslator(source='en', target='hi')\n",
    "            \n",
    "            # Translate the text\n",
    "            translation = translator.translate(word)\n",
    "            \n",
    "            # Print the translated text\n",
    "            print(f\"Original: {word}\")\n",
    "            print(f\"Translated: {translation}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "processor = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized and compiled.\n",
      "Original: Hello, how are you?\n",
      "Translated: नमस्ते, आप कैसे हैं?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processor = preprocess()\n",
    "processor.create_df()\n",
    "processor.drop_cols()\n",
    "processor.clean_text()\n",
    "processor.eng_tokenization()\n",
    "processor.hin_tokenization()\n",
    "processor.vocabulary_creation()\n",
    "processor.seq_padding()\n",
    "processor.split_data()\n",
    "processor.initialize_encoder()\n",
    "processor.initialize_decoder()\n",
    "processor.initialize_model()\n",
    "# processor.initialize_training()\n",
    "text_to_translate = \"Hello, how are you?\"\n",
    "\n",
    "processor.get_output(text_to_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.eng_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54899"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.eng_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
